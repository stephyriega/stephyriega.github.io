<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-01-14T09:49:46-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Franco Calle</title><subtitle></subtitle><author><name>Franco Calle</name></author><entry><title type="html">Discrete Choice Models - Matlab</title><link href="http://localhost:4000/dcm/" rel="alternate" type="text/html" title="Discrete Choice Models - Matlab" /><published>2019-10-15T00:00:00-04:00</published><updated>2019-10-15T00:00:00-04:00</updated><id>http://localhost:4000/dcm</id><content type="html" xml:base="http://localhost:4000/dcm/">&lt;h2 id=&quot;discrete-choice-models-in-matlab&quot;&gt;Discrete choice models in Matlab&lt;/h2&gt;

&lt;p&gt;Fork my repo &lt;a href=&quot;https://github.com/FrancoCalle/DiscreteChoiceModels/blob/master/matlab&quot; target=&quot;blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Models available:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Logit&lt;/li&gt;
  &lt;li&gt;Exploded Logit&lt;/li&gt;
  &lt;li&gt;Logit with Random Coefficients&lt;/li&gt;
  &lt;li&gt;Exploded Logit with Random Coefficients&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Discrete choice models are becoming trendier as more data becomes available in multiple areas like health, education, credit markets, and many others. The key idea of them is to model the process of decision making of an individual for choosing a particular option among a discrete set of option, based on individual and option level characteristics. Many applications of this method come to my mind; for instance, estimating the probability of a family choosing where to enroll their children from an array of schools available near their neighborhood, or estimating the probability of choosing a particular mean of transportation for an individual to get to work.&lt;/p&gt;

&lt;!-- *italics* --&gt;

&lt;p&gt;With the objective of estimating these models, I developed &lt;a href=&quot;https://github.com/FrancoCalle/DiscreteChoiceModels/blob/master/matlab/dcmLab.m&quot; target=&quot;blank&quot;&gt;dcmLab&lt;/a&gt;, a package programed in Matlab, (Python and Julia versions forthcoming) that computes different versions of static discrete choice models that maximize the likelihood, or the joint likelihood, of a decision problem. You can also add non observable heterogeneity to your model by using the random coefficients routine in the package.&lt;/p&gt;

&lt;p&gt;In this post I pretend to briefly explain how the code works in the case of a very simple logistic model without unobservable components. For a description of the mathematical details I recommend reading “Discrete choice methods with simulation” by Kenneth Train 2009.&lt;/p&gt;

&lt;p&gt;The whole package can be found &lt;a href=&quot;https://github.com/FrancoCalle/DiscreteChoiceModels/tree/master/matlab&quot; target=&quot;blank&quot;&gt;here&lt;/a&gt;
The simplest way of testing it is to clone the repository, open matlab and run mainLogit or mainExplodedLogit depending on which model you want to test.&lt;/p&gt;

&lt;p&gt;Disclaimer: I know Matlab’s OOP is very slow and defining all the functions within a single class can be computationally inefficient, however, I decided to have all the code in one class anyways to prevent loading the repo with many .m files. If you’re looking for efficiency, I recommend separating the functions in different .m files, I did that for many other projects and it worked 100% guaranteed.&lt;/p&gt;

&lt;p&gt;Ok, here we go!&lt;/p&gt;

&lt;p&gt;First, the script defines variables entering into the model, in this case we define the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nXX&lt;/code&gt; different explanatory variables and store them in cell array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;XX&lt;/code&gt; which contains arrays of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nObs&lt;/code&gt; observations and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nOptions&lt;/code&gt; options. Our explanatory variables here will be random arrays that comply with the mentioned characteristics but could easily be replaced by characteristics from the real world. All the variables that will be used in the model will be stored in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Data&lt;/code&gt; structure.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nObs = 10000; nOptions = 20; nXX = 15;

Data.nObs = nObs;
Data.nOptions = nOptions;

Data.XX = {};
for ii = 1:nXX
    Data.XX{ii} = randn(nObs,nOptions); %Variable 1
end
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Second, in order to generate data on choices we have to arrange the coefficients that will enter the routine and define the model type we want to be testing. Our logistic model is  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Model 1&lt;/code&gt; and will use random parameters for each X variable. After defining the parameters and the XX variables we can generate fake choices conditional on them using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dcmLab.generateFakeChoices(parameters, false)&lt;/code&gt;. This routine uses as inputs a parameter array and a bool that is true if we want to generate the feasible choice set based on a Deferred Acceptance algorithm (the matlab implementation was adapted by &lt;a href=&quot;https://github.com/mohitkarnani&quot; target=&quot;blank&quot;&gt;Mohit Karnani&lt;/a&gt;), or false if we want to define a random feasible choice set. For this tutorial we will opt for the second. The result of the routine is a new element in Data named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ChoiceIndex&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Pack.Model = 'Model 1'; %This define the type of model to estimate

parameters = dcmLab.setupParameters();
% 1.5. Generate Choices
dcmLab.generateFakeChoices(parameters, false); %Estimate fake data
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;After generating choices, we define the utility function we will use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dcmLab.utilityFunction(parameters,  true)&lt;/code&gt; and check whether the gradients for the objective function of the model are correctly defined. What &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dcmLab.checkGradient(parameters,passedUtilityFunction)&lt;/code&gt; does is compute the difference between the analytic gradient and the numerical gradient that should be zero if the gradient is correctly defined.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;passedUtilityFunction = @(parameters) dcmLab.utilityFunction(parameters,  true);
dcmLab.checkGradient(parameters,passedUtilityFunction)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Once gradients for all parameters were checked, we continue with the optimization procedure. Basically, we introduce our utility function in a handler again, plus some random initial values for our parameters and pass the function to the optimization routine. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dcmLab.estimationNLP&lt;/code&gt; uses the quasi-newton optimization method from Matlab to fit parameters. You can play with the method by changing hyperparameters within dcmLab if you consider necessary.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;passedUtilityFunction = @(theta) dcmLab.utilityFunction(theta, true);
[theta_quasinewton,fobj1,~,~]=dcmLab.estimationNLP(theta_init, passedUtilityFunction);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;This method should perfectly recover the parameters we used to define &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ChoiceIndex&lt;/code&gt; after convergence is reached. Here is a scatterplot that shows how well our estimated coefficients fit to the true parameters.
&lt;img src=&quot;http://localhost:4000/images/dcm/parameter_fitting_no_rc.jpg&quot; alt=&quot;alt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In addition to the Matlab optimization method, I also built an stochastic gradient descent procedure with Adam optimizer (&lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot; target=&quot;blank&quot;&gt;Kingma &amp;amp; Ba 2015&lt;/a&gt;) that can be faster for optimizing objective functions when the dimensionality of the parameter space and/or the number of observations in the model increases. This procedure uses mini batches of data -instead of the whole sample- to estimate the magnitude and direction where parameters should be updated iteration by iteration based on adaptive estimates of lower-order moments. To visualize performance run the next lines of code.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%4.1. Define batch size:
batchSize = 2^10;
nIter = 120;
%4.2. What happens when we start from the true values?
initial_values  = theta_init(1,:); %initial_values  = theta_init(s,:);

passedUtilityFunction = @(theta) dcmLab.utilityFunction(theta, true);
[theta_estimated_Adam, allTheta]= dcmLab.optimizationAlgorithm(initial_values, batchSize, 'Adam', nIter, passedUtilityFunction);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Here we plot how parameters converge after 120 iterations:
&lt;img src=&quot;http://localhost:4000/images/dcm/parameters_convergence_no_rc.jpg&quot; alt=&quot;alt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After parameters were recovered, we can make multiple experiments. For instance, here I added code that simulates how utility would change if we reduce the set of options available to individuals, say, closing schools or closing routes where a car can transit to go to a destination.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nKilled = 5;
Policy = ~(Ranking &amp;gt; nOptions - nKilled);
Data.FeasibleSet = logical(double(Data.FeasibleSet).*double(Policy));

utilities1  = dcmLab.simulateUtilities(parameters);
[~,choice1] = max(utilities1,[],2);

histogram(utilities0(bsxfun(@eq, 1:size(utilities0,2), choice0)), 'EdgeColor', CC(10,:), 'FaceColor', CC(10,:), 'FaceAlpha', 0.4)
hold on
histogram(utilities1(bsxfun(@eq, 1:size(utilities1,2), choice1)), 'EdgeColor', CC(50,:), 'FaceColor', CC(50,:), 'FaceAlpha', 0.4)
legend('Before Policy','After Policy')
grid on
box on
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Evidently, reducing options can reduce overall utility.
&lt;img src=&quot;http://localhost:4000/images/dcm/policy_change.jpg&quot; alt=&quot;alt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I hope this post was useful to understand dcmLab. Please, if you have any comments or want to make changes to improve dcmLab, don’t hesitate on submitting pull requests to the &lt;a href=&quot;https://github.com/FrancoCalle/DiscreteChoiceModels&quot; target=&quot;blank&quot;&gt;repo&lt;/a&gt;. If your changes are approved I’ll publish my gratitude to your contribution to this post and tweet about it.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Bibliography:&lt;/p&gt;

&lt;p&gt;Train, K., &amp;amp; Weeks, M. (2005). &lt;em&gt;Discrete choice models in preference space and willingness-to-pay space&lt;/em&gt;. In Applications of simulation methods in environmental and resource economics (pp. 1-16). Springer, Dordrecht.&lt;/p&gt;

&lt;p&gt;Kingma, D. P., &amp;amp; Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.&lt;/p&gt;

&lt;!-- Python code block:
```python
    import numpy as np

    def test_function(x, y):
      z = np.sum(x,y)
      return z
``` --&gt;</content><author><name>Franco Calle</name></author><category term="logit" /><category term="discrete choice model" /><category term="behavioral model" /><summary type="html">Behavioral Modelling - Python &amp; Matlab</summary></entry></feed>