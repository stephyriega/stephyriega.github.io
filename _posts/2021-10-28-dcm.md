---
title: "Applied Statistics: Causal Inference in Economic Analysis and Machine Learning Methods "
date: 2021-10-28
tags: [ML, Causal Inference,]
header:
excerpt: "R & Python"
mathjax: "false"
---


## Applied Statistics: Causal Inference in Economic Analysis and Machine Learning Methods

You can find my repo [here](https://github.com/stephyriega/ML_CI). In this course we develop a series of proyects and replications in R and Python using Machine Learning in Causal Inference. I will explain about the labs developed here:


* [An inferential problem: The Gender Wage Gap](https://github.com/stephyriega/ML_CI/tree/main/An%20inferential%20problem%20The%20Gender%20Wage%20Gap)

  Using the data from the March Supplement of the U.S. Current Population Survey (2015) and answered the question how to use job-relevant characteristics, such as education and experience, to best predict wages. Now, we focus on the following inference question: What is the difference in predicted wages between men and women with the same job-relevant characteristics? For this, we analyze if there is a difference in the payment of men and women (gender wage gap). The estimated regression coefficient is 0.05% and it measures how our linear prediction of wage changes if we set the gender variable from 0 to 1, holding the controls fixed. We can call this the predictive effect (PE), as it measures the impact of a variable on the prediction we make. When we do partilling out with OLS, we see that the unconditional wage gap of size 8.1% for women increases to about 5.3 after controlling for worker characteristics. Again, the estimated coefficient measures the linear predictive effect (PE) of D on Y after taking out the linear effect of W on both of these variables. This coefficient equals the estimated coefficient from the ols regression with controls. We also add the proof of the Frisch-Waugh-Lovell Theorem.

* [2. Lasso and Partialling-out](https://github.com/stephyriega/ML_CI/tree/main/Potential%20Outcomes%20and%20RCTs)

  The data set we consider is from the March Supplement of the U.S. Current Population Survey, year 2015. The variable of interest Y is the hourly wage rate constructed as the ratio of the annual earnings to the total number of hours worked. In our analysis, we also focus on single (never married) workers. The use of the lasso for partialling-out the basic regressions shows us that the gender gap is of 12.14%, which is different from the 7% gap found in the basic regression using OLS. Probably because lasso supresses some of the regressors but not all of them, and because partialling out controls for the covariates in the both regressions. The use of the lasso for partialling-out the basic regressions shows us that the gender gap is of 11.40%, which is different from the 7% gap found int the basic regression using OLS but pretty similar to the 12% found on the basic regression model with partialling-out. Above the possible reasons already mentioned, in this flexible model we have more regressors but also controlled by the penalty value. We also add the definition and process of Data Splitting.

* [3. Potential Outcomes and RCTs](https://github.com/stephyriega/ML_CI/tree/main/Potential%20Outcomes%20and%20RCTs)

  In this lab, we analyze the Pennsylvania re-employment bonus experiment, which was previously studied in "Sequential testing of duration data: the case of the Pennsylvania ‘reemployment bonus’ experiment" (Bilias, 2000), among others. These experiments were conducted to test the incentive effects of alternative compensation schemes for unemployment insurance (UI). In these experiments, UI claimants were randomly assigned either to a control group or one of five treatment groups and we will focus on treatment group 2. Individuals in the treatment groups were offered a cash bonus if they found a job within some pre-specified period of time (qualification period), provided that the job was retained for a specified duration. The treatments differed in the level of the bonus, the length of the qualification period, and whether the bonus was declining over time in the qualification period. We run classical 2-sample approach, no adjustment (CL), Classical linear regression adjustment (CRA) and Interactive regression adjustment (IRA). IRA regression estimators delivers estimates that are slighly more efficient (lower standard errors) than the simple 2 mean estimator, but essentially all methods have very similar standard errors. From IRA results we also see that there is not any statistically detectable heterogeneity. Also, we explain what is multicollinearity as well as perfect and imperfect multicollinearity. 




## Overall, here is resume of the methods and models that were developed:


Methods available:
- Data splitting
- Partialling out
- Cross validation
- Boostraping

Models available:
- OLS (with RCT data)
- Lasso
- Dobble lasso
- 

