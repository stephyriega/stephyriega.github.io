---
title: "Applied Statistics: Causal Inference in Economic Analysis and Machine Learning Methods "
date: 2021-10-28
tags: [ML, Causal Inference,]
header:
excerpt: "R & Python"
mathjax: "false"
---


## Applied Statistics: Causal Inference in Economic Analysis and Machine Learning Methods

You can find my repo [here](https://github.com/stephyriega/ML_CI). In this course we develop a series of proyects and replications in R and Python using Machine Learning in Causal Inference. I will explain about the labs developed here:


[An inferential problem: The Gender Wage Gap](https://github.com/stephyriega/ML_CI/tree/main/An%20inferential%20problem%20The%20Gender%20Wage%20Gap)

Using the data from the March Supplement of the U.S. Current Population Survey (2015) and answered the question how to use job-relevant characteristics, such as education and experience, to best predict wages. Now, we focus on the following inference question: What is the difference in predicted wages between men and women with the same job-relevant characteristics? For this, we analyze if there is a difference in the payment of men and women (gender wage gap). It may partly reflect discrimination against women in the labor market or may partly reflect a selection effect, namely that women are relatively more likely to take on occupations that pay somewhat less (for example, school teaching). The estimated regression coefficient is 0.05% and it measures how our linear prediction of wage changes if we set the gender variable from 0 to 1, holding the controls fixed. We can call this the predictive effect (PE), as it measures the impact of a variable on the prediction we make. When we do partilling out with OLS, we see that the unconditional wage gap of size 8.1% for women increases to about 5.3 after controlling for worker characteristics. Again, the estimated coefficient measures the linear predictive effect (PE) of D on Y after taking out the linear effect of W on both of these variables. This coefficient equals the estimated coefficient from the ols regression with controls. We also add the proof of  the Frisch-Waugh-Lovell Theorem.



[Potential Outcomes and RCTs](https://github.com/stephyriega/ML_CI/tree/main/Potential%20Outcomes%20and%20RCTs)

In this lab, we analyze the Pennsylvania re-employment bonus experiment, which was previously studied in "Sequential testing of duration data: the case of the Pennsylvania ‘reemployment bonus’ experiment" (Bilias, 2000), among others. These experiments were conducted in the 1980s by the U.S. Department of Labor to test the incentive effects of alternative compensation schemes for unemployment insurance (UI). In these experiments, UI claimants were randomly assigned either to a control group or one of five treatment groups. Actually, there are six treatment groups in the experiments. Here we focus on treatment group 2. In the control group the current rules of the UI applied. Individuals in the treatment groups were offered a cash bonus if they found a job within some pre-specified period of time (qualification period), provided that the job was retained for a specified duration. The treatments differed in the level of the bonus, the length of the qualification period, and whether the bonus was declining over time in the qualification period. We run classical 2-sample approach, no adjustment (CL), Classical linear regression adjustment (CRA) and Interactive regression adjustment (IRA). Treatment group 2 experiences an average decrease of about $7.26\%$ in the length of unemployment spell. Observe that IRA regression estimators delivers estimates that are slighly more efficient (lower standard errors) than the simple 2 mean estimator, but essentially all methods have very similar standard errors. From IRA results we also see that there is not any statistically detectable heterogeneity. We also see the regression estimators offer slightly lower estimates -- these difference occur perhaps to due minor imbalance in the treatment allocation, which the regression estimators try to correct.


[Potential Outcomes and RCTs](https://github.com/stephyriega/ML_CI/tree/main/Potential%20Outcomes%20and%20RCTs)


The data set we consider is from the March Supplement of the U.S. Current Population Survey, year 2015. The variable of interest Y is the hourly wage rate constructed as the ratio of the annual earnings to the total number of hours worked, which is constructed in turn as the product of number of weeks worked and the usual number of hours worked per week. In our analysis, we also focus on single (never married) workers. The use of the lasso for partialling-out the basic regressions shows us that the gender gap is of 12.14%, which is different from the 7% gap found int the basic regression using OLS. Probably because lasso supresses some of the regressors but not all of them, and because partialling out controls for the covariates in the both regressions. The use of the lasso for partialling-out the basic regressions shows us that the gender gap is of 11.40\%, which is different from the $7\%$ gap found int the basic regression using OLS but pretty similar to the 12\% found on the basic regression model with partialling-out. Above the possible reasons already mentioned, in this flexible model we have more regressors but also controlled by the penalty value. We also add the definition and process of Data Splitting.


## Overall, here is resume of the methods and models that were developed:


Methods available:
- Data splitting
- Partialling out
- Cross validation
- Boostraping

Models available:
- OLS (with RCT data)
- Lasso
- Dobble lasso
- 

