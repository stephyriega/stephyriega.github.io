---
title: "Discrete Choice Models - Matlab"
date: 2018-01-28
tags: [logit, discrete choice model, behavioral model]
header:
excerpt: "Behavioral Modelling - Python & Matlab"
mathjax: "true"
---


## Package for estimation and simulation of discrete choice models [Matlab]


Discrete choice models are becoming trendier as the availability of data increases in multiple areas like health, education, credit markets, and many others. The key idea of them is to model the decision making of an individual to choose a particular option among a discrete set of them, based on individual and option level characteristics. An application of this method could be to estimate the probability of a family choosing where to enroll their kids from an array of schools available near their neighborhood, or estimate the probability of choosing a particular mean of transportation for an individual to get to work.

<!-- *italics* -->

I developed a package in Matlab, (Python and Julia versions forthcoming) that estimates different static versions of discrete choice models that maximize the likelihood or the joint likelihood of different decision problems. In this blog post I pretend to explain the code and how it works but not the mathematical details of it, for this I recommend reading  Discrete choice methods with simulation by Kenneth Train 2009.

The whole package for my code can be found [here](https://github.com/FrancoCalle/DiscreteChoiceModels/tree/master/matlab)
The most simple way of testing it is to clone the repository, open matlab and run mainLogit. I know that Matlab is an object oriented language and defining all the functions within a single class can be very computationally inefficient, however, I decided to have all the code in one class anyways to prevent the repo to be overload with many .m files. If you're looking for more efficiency, I recommend separating the functions in different .m files, I did that for other project and it worked 100% guaranteed.

Ok, here we go!

<!-- Python code block:
```python
    import numpy as np

    def test_function(x, y):
      z = np.sum(x,y)
      return z
``` -->

```
Data.XX = {};
for ii = 1:nXX
    Data.XX{ii} = randn(nStudents,nOptions); %Variable 1
end
```

```
Pack.Model = 'Model 1'; %This define the type of model to estimate

parameters = dcmLab.setupParameters();
% 1.5. Generate Choices
dcmLab.generateFakeChoices(parameters, false); %Estimate fake data
```


```
passedUtilityFunction = @(parameters) dcmLab.utilityFunction(parameters,  true);
dcmLab.checkGradient(parameters,passedUtilityFunction)
```

```
passedUtilityFunction = @(theta) dcmLab.utilityFunction(theta, true);
[theta_quasinewton,fobj1,~,~]=dcmLab.estimationNLP(theta_init, passedUtilityFunction);
```

```
%4.1. Define batch size:
batchSize = 2^10;
nIter = 120;
%4.2. What happens when we start from the true values?
initial_values  = theta_init(1,:); %initial_values  = theta_init(s,:);

passedUtilityFunction = @(theta) dcmLab.utilityFunction(theta, true);
[theta_estimated_Adam, allTheta]= dcmLab.optimizationAlgorithm(initial_values, batchSize, 'Adam', nIter, passedUtilityFunction);
```


Here's some inline code `x+y`.

<!-- Here's an image:
<img src="{{ site.url }}{{ site.baseurl }}/images/dcm/parameters_convergence_no_rc.jpg" alt="linearly separable data"> -->

How parameters converge after 120 iterations using stochastic gradient descent:
![alt]({{ site.url }}{{ site.baseurl }}/images/dcm/parameters_convergence_no_rc.jpg)

Here are some simulations of option removal. Evidently, less options imply less utility.
![alt]({{ site.url }}{{ site.baseurl }}/images/dcm/policy_change.jpg)


Here's some math:

$$z=x+y$$

You can also put it inline $$z=x+y$$



Bibliography:

Train, K., & Weeks, M. (2005). Discrete choice models in preference space and willingness-to-pay space. In Applications of simulation methods in environmental and resource economics (pp. 1-16). Springer, Dordrecht.
